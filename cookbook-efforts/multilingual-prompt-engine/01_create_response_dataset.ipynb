{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "import markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import subprocess\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "HfFolder.save_token(hf_token)\n",
    "\n",
    "\n",
    "result = subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_token], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Successfully logged in to Hugging Face.\")\n",
    "else:\n",
    "    print(\"Failed to log in to Hugging Face.\")\n",
    "    print(result.stderr.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DIBT/MPEP_DUTCH\n",
      "External ID: 1788\n",
      "Value: Als een AI-enthousiasteling, houd je ervan om programma's te maken die de menselijke taal begrijpen. Je nieuwste project is een programma dat woorden kan herkennen en vervangen door hun antoniemen in een stuk tekst.\n",
      "Om de effectiviteit van je programma aan te tonen, besluit je het te testen op een nieuwsartikel over een recent politiek evenement. Om het uitdagender te maken, wil je ook dat je programma onderscheid maakt tussen homoniemen, en daarnaast contextuele aanwijzingen gebruikt woorden correct te vervangen.\n",
      "Hier is een stapsgewijze uitleg van hoe je programma werkt:\n",
      "1. Het programma leest de inputtekst en identificeert alle woorden die antoniemen hebben.\n",
      "2. Voor elk van deze woorden identificeert het programma de context waarin het verschijnt om het juiste antoniem te bepalen.\n",
      "3. Het programma vervangt vervolgens het originele woord door zijn antoniem in de tekst.\n",
      "4. Als het originele woord meerdere betekenissen heeft, gebruikt het programma de context om te bepalen welke betekenis bedoeld is en vervangt het door het passende antoniem.\n",
      "5. Uiteindelijk geeft het programma de aangepaste tekst uit met de vervangen woorden.\n",
      "Kun je je vaardigheden in natuurlijke taalverwerking op de proef stellen en proberen de antoniemen te identificeren die in de gewijzigde tekst worden gebruikt?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_SPANISH\n",
      "External ID: 165\n",
      "Value: Dado el texto: Una innovadora experimentada y entusiasta... que quieres en tu equipo.\n",
      "Margaret Hines es la fundadora y Consultora Principal de Inspire Marketing, LLC, que invierte en negocios locales, sirviendo a la comunidad con consultoría de negocios y marketing. Ella tiene un título universitario de la Universidad de Washington en St. Louis, MO, y un MBA de la Universidad de Wisconsin-Milwaukee.\n",
      "Margaret ofrece consultoría en marketing, ventas de negocios, transformaciones de negocios y franquicias. También es inversora en negocios locales.\n",
      "Antes de fundar Inspire Marketing en 2003, Margaret adquirió su habilidad para los negocios, experiencia en ventas y marketing mientras trabajaba en respetadas empresas de Fortune 1000.\n",
      "Resume la formación y experiencia de Margaret Hines, la fundadora de Inspire Marketing.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_MALAGASY\n",
      "External ID: 944\n",
      "Value: Mila CV aho ho an'ity asa ity\n",
      "Junior PHP injeniera (f / m / d)\n",
      "Momba ny asa\n",
      "Anstellungsdetails\n",
      "\n",
      "Fifanekena maharitra, fotoana feno na tapak'andro, Cologne / Düsseldorf / Darmstadt / Remote (any Alemaina)\n",
      "\n",
      "Info\n",
      "\n",
      "Iza moa isika\n",
      "\n",
      "Miasa Kaufland.de izahay: mpivarotra an'arivony sy vokatra an-tapitrisany no mahatonga antsika ho iray amin'ireo tsena an-tserasera mitombo haingana indrindra. Ny asanay dia miavaka amin'ny kolontsain'ny orinasa mavitrika, miaraka amin'ny toe-tsaina manomboka sy ny herin'ny vondrona orinasa lehibe. Manambatra ny fahalalana sy ny traikefa an-taonany maro amin'ny e-varotra miaraka amin'ny ambaratongam-pahefana sy ekipa tena manosika izahay. Na avy amin'ny intern: Raisinay ho zava-dehibe ny hevitra rehetra, satria te-hiara-hiasa amin'ny famolavolana ny hoavin'ny e-varotra izahay!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_GERMAN\n",
      "External ID: 5996\n",
      "Value: Kannst du zwei Beispiele für Metaphern im Excel-Tabellenformat liefern?\n",
      "\n",
      "Hier ist eine Excel-Tabelle mit zwei Beispielen für Metaphern:\n",
      "\n",
      "| Metapher        | Bedeutung           |\n",
      "\n",
      "| ------------- |:-------------:|\n",
      "\n",
      "| Das Leben ist eine Reise | Das Leben kann mit einer Reise mit Höhen und Tiefen verglichen werden |\n",
      "\n",
      "| Liebe ist wie eine Rose  | Liebe kann mit einer zarten und schönen Rose verglichen werden |\n",
      "\n",
      "Kannst du zwei weitere Beispiele für Metaphern in einem MATLAB-Codeformat liefern?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_SWAHILI\n",
      "External ID: 348\n",
      "Value: Ungefafanuaje manyoya ya mbwa wa mlima wa Uswisi?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_FILIPINO\n",
      "External ID: 1752\n",
      "Value: ano ang papel na ginagampanan ng pass transistor at op amp sa isang regulator\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_ARABIC\n",
      "External ID: None\n",
      "Value: إذا كانت الوصفة تتطلب كوبين ونصف من السكر وتريد تحضير نصف هذه الكمية، فاحسب كمية السكر المطلوبة بالضبط.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_CZECH\n",
      "External ID: 1804\n",
      "Value: Jaký je vliv změny klimatu na polární ledové čepice a jak ovlivňuje globální hladinu moří?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_HUNGARIAN\n",
      "External ID: 9999\n",
      "Value: Mint mesterséges intelligencia rajongó, szeretsz olyan programokat készíteni, amelyek képesek megérteni az emberi nyelvet. Legújabb projekted egy olyan program fejlesztése, amely képes felismerni és kicserélni a szavakat azok ellentéteire egy adott szövegben.\n",
      "Annak érdekében, hogy bemutasd a program hatékonyságát, úgy döntesz, hogy teszteled azt egy újságcikken, amely egy nemrégiben történt politikai eseményről szól. Azonban, hogy még nagyobb kihívást jelentsen, azt is szeretnéd, ha a program megkülönböztetné a homonimákat és a kontextus alapján helyesen cserélné ki azokat.\n",
      "Íme, egy lépésről-lépésre leírás a program működéséről:\n",
      "1. A program beolvassa a bemeneti szöveget és azonosít minden olyan szót, amelynek van ellentéte.\n",
      "2. Minden ilyen szó esetén a program azonosítja a kontextust, amelyben megjelennek, hogy meghatározza a helyes ellentétes szót, amit használni kell.\n",
      "3. A program ezután kicseréli az eredeti szót annak ellentetjére a szövegben.\n",
      "4. Ha az eredeti szónak több jelentése is van, a program a kontextust használja annak meghatározására, hogy melyik jelentés értendő, és kicseréli a megfelelő ellentétes szóra.\n",
      "5. Végül a program visszaadja a módosított szöveget a kicserélt szavakkal.\n",
      "Próbára tennéd a természetes nyelvfeldolgozási képességeidet, hogy azonosítsd a módosított szövegben használt ellentéteket?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_RUSSIAN\n",
      "External ID: 165\n",
      "Value: Учитывая текст: Опытный и энтузиастичный новатор... вы хотите в своей команде. Маргарет Хайнс является основателем и главным консультантом Inspire Marketing, LLC, инвестирующей в местные предприятия, обслуживающей сообщество бизнес-брокером и маркетинговым консультированием. Она имеет степень бакалавра в Вашингтонском университете в Сент-Луисе, штат Москва, и MBA из Университета Висконсинка-Милваки. Маргарет предлагает консультации в области маркетинга, продаж бизнеса и поворотов и франчайзинга.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Dataset: DIBT/MPEP_GREEK\n",
      "External ID: 888\n",
      "Value: Βάσει του κειμένου: Μία έμπειρη και ενθουσιώδης καινοτόμος... που θέλετε στην ομάδα σας.\n",
      "Η Margaret Hines είναι η ιδρύτρια και η κύρια σύμβουλος της Inspire Marketing, LLC, έχοντας επενδύσει σε τοπικές επιχειρήσεις, εξυπηρετώντας την κοινότητα μέσω επιχειρηματικής μεσιτείας και συμβουλών μάρκετινγκ. Έχει πτυχίο από το Πανεπιστήμιο της Ουάσιγκτον στο St. Louis, MO, και MBA από το Πανεπιστήμιο του Wisconsin-Milwaukee.\n",
      "Η Margaret προσφέρει συμβουλές σε θέματα μάρκετινγκ, επιχειρηματικών πωλήσεων και ανακατασκευών και franchising. Είναι επίσης επενδύτρια σε τοπικές επιχειρήσεις.\n",
      "Πριν από την ίδρυση της Inspire Marketing το 2003, η Margaret απέκτησε την επιχειρηματική της οξυδέρκεια, και την τεχνογνωσία της στις πωλήσεις και το μάρκετινγκ όσο εργαζόταν σε αναγνωρισμένες εταιρείες του Fortune 1000.\n",
      "Συνόψισε το ιστορικό και την τεχνογνωσία της Margaret Hines, της ιδρύτριας του Inspire Marketing.\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect and print MPEP datasets\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def search_datasets_by_label(label):\n",
    "    datasets = api.list_datasets(search=label)\n",
    "    return datasets\n",
    "\n",
    "mpep_datasets = search_datasets_by_label('MPEP')\n",
    "\n",
    "# Extract and print the first entry's value parameter from the target JSON object and external_id for each dataset\n",
    "for dataset in mpep_datasets:\n",
    "    ds = load_dataset(dataset.id)\n",
    "    target_entries = ds['train']['target'][:1]\n",
    "    external_ids = ds['train']['external_id'][:1]\n",
    "    print(f\"Dataset: {dataset.id}\")\n",
    "    if target_entries:\n",
    "        first_target = target_entries[0]\n",
    "        if isinstance(first_target, list) and first_target:\n",
    "            value = first_target[0].get('value', 'No value found')\n",
    "            print(f\"External ID: {external_ids[0]}\")\n",
    "            print(f\"Value: {value}\")\n",
    "        elif isinstance(first_target, dict):\n",
    "            value = first_target.get('value', 'No value found')\n",
    "            print(f\"External ID: {external_ids[0]}\")\n",
    "            print(f\"Value: {value}\")\n",
    "        else:\n",
    "            print(\"No target value found\")\n",
    "    else:\n",
    "        print(\"No target entries found\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hf_inference(model, prompt, token):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json={\"inputs\": prompt}, timeout=30)\n",
    "        # Check for valid response\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result = response.json()\n",
    "                if isinstance(result, list) and len(result) > 0:\n",
    "                    return result[0].get('generated_text', 'No generated text found')\n",
    "                else:\n",
    "                    return \"Unexpected response format\"\n",
    "            except ValueError:\n",
    "                return \"Invalid JSON response\"\n",
    "        else:\n",
    "            return f\"Error {response.status_code}: {response.text}\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Request timed out\"\n",
    "\n",
    "\n",
    "# List of target models\n",
    "models = [\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"Qwen/Qwen2-72B-Instruct\",\n",
    "    \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "]\n",
    "\n",
    "# Number of prompts to process\n",
    "num_prompts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 1162 as it is already processed.\n",
      "Skipping prompt with external_id: 765 as it is already processed.\n",
      "Skipping prompt with external_id: 964 as it is already processed.\n",
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 1162 as it is already processed.\n",
      "Skipping prompt with external_id: 765 as it is already processed.\n",
      "Skipping prompt with external_id: 964 as it is already processed.\n",
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 1162 as it is already processed.\n",
      "Skipping prompt with external_id: 765 as it is already processed.\n",
      "Skipping prompt with external_id: 964 as it is already processed.\n",
      "Skipping dataset DIBT/MPEP_SPANISH as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_MALAGASY as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_GERMAN as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_SWAHILI as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_FILIPINO as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_ARABIC as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_CZECH as it has less than 500 prompts.\n",
      "Skipping dataset DIBT/MPEP_HUNGARIAN as it has less than 500 prompts.\n",
      "Skipping prompt with external_id: 165 as it is already processed.\n",
      "Skipping prompt with external_id: 935 as it is already processed.\n",
      "Skipping prompt with external_id: 918 as it is already processed.\n",
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 165 as it is already processed.\n",
      "Skipping prompt with external_id: 935 as it is already processed.\n",
      "Skipping prompt with external_id: 918 as it is already processed.\n",
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 165 as it is already processed.\n",
      "Skipping prompt with external_id: 935 as it is already processed.\n",
      "Skipping prompt with external_id: 918 as it is already processed.\n",
      "Skipping prompt with external_id: 1788 as it is already processed.\n",
      "Skipping prompt with external_id: 1752 as it is already processed.\n",
      "Skipping prompt with external_id: 888 as it is already processed.\n",
      "Skipping prompt with external_id: 4565 as it is already processed.\n",
      "Skipping prompt with external_id: 4482 as it is already processed.\n",
      "Skipping prompt with external_id: 9999 as it is already processed.\n",
      "Skipping prompt with external_id: 9554 as it is already processed.\n",
      "Skipping prompt with external_id: 888 as it is already processed.\n",
      "Skipping prompt with external_id: 4565 as it is already processed.\n",
      "Skipping prompt with external_id: 4482 as it is already processed.\n",
      "Skipping prompt with external_id: 9999 as it is already processed.\n",
      "Skipping prompt with external_id: 9554 as it is already processed.\n",
      "Skipping prompt with external_id: 888 as it is already processed.\n",
      "Skipping prompt with external_id: 4565 as it is already processed.\n",
      "Skipping prompt with external_id: 4482 as it is already processed.\n",
      "Skipping prompt with external_id: 9999 as it is already processed.\n",
      "Skipping prompt with external_id: 9554 as it is already processed.\n",
      "Script completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def search_datasets_by_label(label):\n",
    "    # Search datasets with the specified label\n",
    "    datasets = api.list_datasets(search=label)\n",
    "    return datasets\n",
    "\n",
    "# Search for datasets with the \"MPEP\" label\n",
    "mpep_datasets = search_datasets_by_label('MPEP')\n",
    "\n",
    "# Directories to save responses and cache\n",
    "os.makedirs('./responses', exist_ok=True)\n",
    "os.makedirs('./cache', exist_ok=True)\n",
    "\n",
    "# Load existing cache files\n",
    "processed_external_ids = {}\n",
    "for cache_file in os.listdir('./cache'):\n",
    "    if cache_file.endswith('.txt'):\n",
    "        language = cache_file.replace('.txt', '')\n",
    "        with open(f'./cache/{cache_file}', 'r') as f:\n",
    "            processed_external_ids[language] = set(f.read().splitlines())\n",
    "\n",
    "# Process each dataset\n",
    "for dataset in mpep_datasets:\n",
    "    # Load the dataset\n",
    "    ds = load_dataset(dataset.id)\n",
    "    # Check if the dataset has 500 or more prompts\n",
    "    if len(ds['train']) < 500:\n",
    "        print(f\"Skipping dataset {dataset.id} as it has less than 500 prompts.\")\n",
    "        continue\n",
    "    \n",
    "    # Extract the first 'num_prompts' entries from the target JSON object\n",
    "    target_entries = ds['train']['target'][:num_prompts]\n",
    "    external_ids = ds['train']['external_id'][:num_prompts]\n",
    "    \n",
    "    # Get the language from the dataset id (assuming the format is consistent)\n",
    "    language = dataset.id.split('_')[-1].lower()\n",
    "    \n",
    "    # Initialize the cache set for the language if not already done\n",
    "    if language not in processed_external_ids:\n",
    "        processed_external_ids[language] = set()\n",
    "    \n",
    "    # Open a file to write the responses\n",
    "    with open(f'./responses/{language}.md', 'w') as file:\n",
    "        # Capture responses for each model\n",
    "        for model in models:\n",
    "            file.write(f\"# Model: {model}\\n\\n\")\n",
    "            for i, target_entry in enumerate(target_entries):\n",
    "                external_id = external_ids[i]\n",
    "                # Check if this external_id has already been processed\n",
    "                if external_id in processed_external_ids[language]:\n",
    "                    print(f\"Skipping prompt with external_id: {external_id} as it is already processed.\")\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(target_entry, list) and target_entry:\n",
    "                    prompt = target_entry[0].get('value', 'No value found')\n",
    "                elif isinstance(target_entry, dict):\n",
    "                    prompt = target_entry.get('value', 'No value found')\n",
    "                else:\n",
    "                    prompt = \"No prompt found\"\n",
    "                \n",
    "                response_text = query_hf_inference(model, prompt, os.getenv('HF_TOKEN'))\n",
    "                \n",
    "                file.write(f\"## Prompt:\\n{prompt}\\n\\n\")\n",
    "                file.write(f\"## Response:\\n{response_text}\\n\\n\")\n",
    "                file.write(\"\\n\" + \"-\"*40 + \"\\n\\n\")\n",
    "                \n",
    "                # Add the processed external_id to the set\n",
    "                processed_external_ids[language].add(external_id)\n",
    "    \n",
    "    # Write all processed external_ids for this language to the cache file\n",
    "    with open(f'./cache/{language}.txt', 'w') as cache_file:\n",
    "        for external_id in processed_external_ids[language]:\n",
    "            cache_file.write(f\"{external_id}\\n\")\n",
    "\n",
    "print(\"Script completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
